# -*- coding: utf-8 -*-
"""salary prediction 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jn3YCv_B3-37UqsDShN4MmKwYQUFKrg_
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn as sns

data= pd.read_csv('/content/Salary_Data.csv')
data.head()

data["Education Level"]

data["Job Title"]

data.tail()

data.shape

data.info()

data.isnull().sum()

data['Gender'].fillna(data['Gender'].mode()[0], inplace=True)
data['Age'].fillna(data['Age'].mode()[0], inplace=True)
data['Education Level'].fillna(data['Education Level'].mode()[0], inplace=True)
data['Job Title'].fillna(data['Job Title'].mode()[0], inplace=True)
data['Salary'].fillna(data['Salary'].mode()[0], inplace=True)
data['Years of Experience'].fillna(data['Years of Experience'].mode()[0], inplace=True)

#missing value

data.isnull().sum()

#find duplicates

data.duplicated().sum()

#duplicate treatment


data.drop_duplicates(inplace=True)

data.duplicated().sum()

#find garbage value
for i in data.select_dtypes(include='object').columns:

  print(data[i].value_counts())
  print("***"*10)

data.size

# Replace 'Other' with 'Male'
data['Gender'] = data['Gender'].replace('Other', 'Male')

# Any values that are not 'Male' will be replaced with 'Female'
data['Gender'] = data['Gender'].apply(lambda x: 'Male' if x == 'Male' else 'Female')

data.nunique()

data['Education Level'].unique()

"""# EDA [EXPLORATORY DATA ANALYSIS]"""

data.describe()

import seaborn as sns
import matplotlib.pyplot as plt
#histogram to understand the distribution

for i in data.select_dtypes(include='number').columns:
  sns.histplot(data=data,x=i,kde=True)
  plt.show()

#boxplot to identify outliers


col = ['Age','Years of Experience','Salary']

for i in col:
  plt.figure()
  plt.boxplot(data['Salary'])
  plt.title(i)
  plt.show()

# scater plot to understand the relationship


data.select_dtypes(include="number").columns

for i in ['Age', 'Years of Experience']:
    sns.scatterplot(data=data,x=i,y='Salary')
    plt.show()

data.select_dtypes(include="number").corr

#correlation with heatmap to interpret the relation and multicolliniarity

data.corr

data.select_dtypes(include="number").corr

plt.figure(figsize=(10,10))
sns.heatmap(data.select_dtypes(include="number").corr(),annot=True) # Select only numerical columns for correlation
plt.show()

"""# visualization"""

top_10_job_titles = data['Job Title'].value_counts().head(10)

plt.figure(figsize=(14, 8))
bars = top_10_job_titles.plot(kind='bar', color='mediumblue')
plt.title('Top 10 Job Titles ', fontsize=25)
plt.xlabel('Job Title', fontsize=20)
plt.ylabel('Employees', fontsize=20)
plt.xticks(rotation=45, ha='right')

for bar in bars.patches:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height, int(height), ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.show()

top_salaries = data.groupby('Job Title')['Salary'].mean()
top_10_salaries = top_salaries.sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 8))
bars = top_10_salaries.plot(kind='bar', color="green")
plt.title('Top 10 Job Titles by Average Salary $' , fontsize=23)
plt.xlabel('Job Title', fontsize=20)
plt.ylabel('Average Salary' , fontsize=18)
plt.xticks(rotation=45, ha='right')

for bar in bars.patches:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, height, int(height), ha='center', va='bottom', fontsize=10)

plt.tight_layout()

plt.show()

edu_level = data["Education Level"].value_counts()
(edu_level / sum(edu_level)) * 100

plt.pie(edu_level, labels=edu_level.index, autopct='%1.1f%%')
plt.title('Distribution of eduacatin level')
plt.show()

top_10_job_titles = data['Job Title'].value_counts().head(10).index
filtered_df = data[data['Job Title'].isin(top_10_job_titles)]
grouped_data = filtered_df.groupby(['Job Title', 'Education Level'])['Salary'].mean().unstack()

grouped_data.plot(kind='bar', figsize=(14, 8), width=0.8)
plt.title('Average Salary by Job Title and Education  Level (Top 10 Job Titles)', fontsize=18)
plt.xlabel('Job Title', fontsize=16)
plt.ylabel('Average Salary $', fontsize=14)
plt.xticks(rotation=45, ha='right')
plt.legend(title='Experience Level', fontsize=10)
plt.tight_layout()
plt.show()

average_salary_employment = data.groupby('Years of Experience')['Salary'].mean()

plt.figure(figsize=(12, 8))
average_salary_employment.sort_values().plot(kind='bar', color='blue')
plt.title('Average Salary by Years of Experience' , fontsize=20)
plt.xlabel('Average Salary' , fontsize=16)
plt.ylabel('Years of Experience' , fontsize=16)
plt.xticks(rotation=0)
plt.show()

average_salary_by_year = data.groupby('Gender')['Salary'].mean().reset_index()

plt.figure(figsize=(10, 6))
plt.plot(average_salary_by_year['Gender'], average_salary_by_year['Salary'], marker='o',color="black")
plt.title('Average Salary by Gender' , fontsize=20)
plt.xlabel('Gender' , fontsize=16)
plt.ylabel('Average Salary )' , fontsize=14)
plt.grid(True)
plt.show()

data["Gender"].value_counts()

plt.figure(figsize=(12, 6))
sns.histplot(data['Salary'], kde=True,color="violet",edgecolor="black")
plt.legend()
plt.title("The Dustribution of Salary" , fontsize=20)
plt.xlabel("Salary $" , fontsize=16)
plt.ylabel("Frequency" , fontsize=16)
plt.show()

data

data["Education Level"].unique()

"""# ENCODING THE DATA"""

!pip install scikit-learn
from sklearn.preprocessing import LabelEncoder

lab_enco=LabelEncoder()
cols=['Gender','Education Level','Job Title']
for i in cols:
  data[i]=lab_enco.fit_transform(data[i]) # Encode categorical features

data

data["Education Level"].unique()

data["Education Level"]

import matplotlib.pyplot as plt
import seaborn as sns

#Calculate the correlation matrix (after converting to numeric)
correlation_matrix = data.corr()

 #Create a heatmap
plt.figure(figsize=(15,15))  # Adjust figure size as needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

"""# scalling"""

from sklearn.preprocessing import MinMaxScaler

# Assuming 'data1' is your DataFrame with numerical features you want to scale
numerical_features = ['Age', 'Years of Experience','Job Title','Education Level','Gender']
scaler = MinMaxScaler()

# Fit the scaler to your data and transform it
data[numerical_features] = scaler.fit_transform(data[numerical_features])

data

X = data.drop("Salary", axis=1)
y = data["Salary"]  # target variable

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

"""# linear regression"""

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(y_pred)

from sklearn.metrics import mean_absolute_error, r2_score

R2 = r2_score(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
print("R2 Score: ", R2)
print("Mean Absolute Error: ", MAE)

"""# Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

rfr = RandomForestRegressor()
rfr.fit(X_train, y_train)

y_pred = rfr.predict(X_test)
y_pred

from sklearn.metrics import mean_absolute_error, r2_score

R2 = r2_score(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
print("R2 Score: ", R2)
print("Mean Absolute Error: ", MAE)

"""# Support Vector Regression (SVR)"""

from sklearn.svm import SVR

svr = SVR()
svr.fit(X_train, y_train)

y_pred = svr.predict(X_test)
y_pred

from sklearn.metrics import mean_absolute_error, r2_score

R2 = r2_score(y_test, y_pred)
MAE = mean_absolute_error(y_test, y_pred) # Use mean_absolute_error instead of mean_squared_error
print("R2 Score: ", R2)
print("Mean Absolute Error: ", MAE)

"""# Gradient Boosting Regressor"""

import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor

# Instantiate the Gradient Boosting Regressor
gb_model = GradientBoostingRegressor(random_state=42)

# Fit the model to the training data
gb_model.fit(X_train, y_train)

# Predict on the test data
y_pred_gb = gb_model.predict(X_test)



R2 = r2_score(y_test, y_pred_gb)
MAE = mean_absolute_error(y_test, y_pred_gb) # Use mean_absolute_error instead of mean_squared_error
print("R2 Score: ", R2)
print("Mean Absolute Error: ", MAE)

"""# XG Boost Regressor"""

import matplotlib.pyplot as plt
import xgboost as xgb

# Create an XGBoost regressor
xgb_model = xgb.XGBRegressor(random_state=42)

# Fit the model to the training data
xgb_model.fit(X_train, y_train)

# Predict on the test data
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
from sklearn.metrics import mean_absolute_error, r2_score

R2 = r2_score(y_test, y_pred_xgb)
MAE = mean_absolute_error(y_test, y_pred_xgb) # Use mean_absolute_error instead of mean_squared_error
print("R2 Score: ", R2)
print("Mean Absolute Error: ", MAE)

"""(Hyperparameter tuning) *GridSearchCV"""

# from sklearn.model_selection import GridSearchCV

# n_estimators = [50, 100, 200, 300]
# max_depth = [3, 5, 7, 9]
# learning_rate = [0.01, 0.1, 0.2]
# subsample = [0.8, 0.9, 1.0]
# colsample_bytree = [0.8, 0.9, 1.0]
# gamma = [0, 0.1, 0.2]

# param_grid = {
#     'n_estimators':n_estimators,
#     'max_depth': max_depth,
#     'learning_rate': learning_rate,
#     'subsample': subsample,
#     'colsample_bytree': colsample_bytree,
#     'gamma':gamma
# }
# print(param_grid)

# rf_model =  xgb.XGBRegressor()

# rf_grid = GridSearchCV(estimator = rf_model, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)

# Fit the RandomizedSearchCV object to the training data
# rf_grid.fit(X_train,y_train)

# Access the best parameters using the correct attribute name
# rf_grid.best_params_

# rf_grid.best_score_

#  import matplotlib.pyplot as plt
#  import xgboost as xgb


#   #Create the model with the desired hyperparameters
#  best_model =xgb.XGBRegressor(
#  colsample_bytree= 0.8,
#  gamma= 0,
#  learning_rate= 0.1,
#  max_depth= 5,
#  n_estimators= 200,
#  subsample= 1.0)
#   #Fit the model
# best_model.fit(X_train, y_train)

#rf_grid.best_params_



#rf_model.best_params_

# Now you can access the best_score_ attribute
#best_model.score(X_test,y_test)

# import pickle
# with open('model.pkl','wb') as files:
#   pickle.dump(xgb.XGBRegressor,files)